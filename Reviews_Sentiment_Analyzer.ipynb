{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Sentiment Analyzer\n",
    "Xuan Chew\n",
    "\n",
    "May 8, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Paths\n",
    "# Movie Reviews\n",
    "POS_FILE = 'Data/pos'\n",
    "NEG_FILE = 'Data/neg'\n",
    "NRC_FILE = 'Data/NRC.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from os import listdir\n",
    "import os\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xuanchew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuanchew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Setup\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie File Reading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a file and read the text\n",
    "def readFile(file):\n",
    "    reader = open(file, 'r')\n",
    "    inputText = reader.read()\n",
    "    reader.close()\n",
    "    \n",
    "    return inputText\n",
    "\n",
    "# Read and clean text as needed\n",
    "def readAndClean(path, counter):\n",
    "    for file in listdir(path):\n",
    "        fullPath = path + '/' + file\n",
    "        text = readFile(fullPath)\n",
    "        improved = cleanText(text)\n",
    "        counter.update(improved)\n",
    "        \n",
    "#Helper function that gets a filename, reads and process the text\n",
    "def getTokens(filename):\n",
    "    text = readFile(filename)\n",
    "    tokens = cleanText(text)\n",
    "    return tokens\n",
    "\n",
    "def prepFiles(pathDict, polarity):\n",
    "    result = []\n",
    "    \n",
    "    for filename in os.listdir(pathDict):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = pathDict + \"/\" + filename\n",
    "            tokens = getTokens(path)\n",
    "            item = (tokens, polarity)\n",
    "            result.append(item)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove stopwords\n",
    "def removeStopWords(text):\n",
    "    tokens = text.split()\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    return ([ word for word in tokens if word not in stopWords])\n",
    "\n",
    "# Removing single character words\n",
    "def removeSingleChar(text):\n",
    "    tokens = text.split()\n",
    "    return ([word for word in tokens if len(word) > 1])\n",
    "\n",
    "# Removing punctuation\n",
    "def removePunctuation(text):\n",
    "    tokens = text.split()\n",
    "    transTable = str.maketrans('','', string.punctuation)\n",
    "    return ([word.translate(transTable) for word in tokens])\n",
    "\n",
    "# Removing non alphabetic characters\n",
    "def removeNonAlpha(text):\n",
    "    tokens = text.split()\n",
    "    return ([ word for word in tokens if word.isalpha()])\n",
    "\n",
    "# Primary cleaning function\n",
    "def cleanText(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Prepare tokens by splitting with text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Lemmatize\n",
    "    for token in tokens:\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "    \n",
    "    # Cleanup by removing stopwords based on stop words corpora\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    tokens = [ word for word in tokens if word not in stopWords]\n",
    "    \n",
    "    # Cleanup by removing single char words\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "\n",
    "    # Cleanup data by removing punctuation\n",
    "    punctuation = string.punctuation\n",
    "    transTable = str.maketrans('','',punctuation)\n",
    "    tokens = [word.translate(transTable) for word in tokens]\n",
    "\n",
    "    # Filter by only keeping alphabets\n",
    "    tokens = [ word for word in tokens if word.isalpha()]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline : Basic Score Based Model\n",
    "This baseline model utilizes the NRC EmoLex Lexicon in a very simple way that naively assumes that text with more positive words will most likely carry an overall positive sentiment and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotionDataScore():\n",
    "    scoreDict = {}\n",
    "    \n",
    "    #NRC Prep\n",
    "    emoLines = open(NRC_FILE).readlines()\n",
    "    strippedEmoLines = [x.strip() for x in emoLines]\n",
    "    cleanedEmoLines = [x.replace('\\t',',') for x in strippedEmoLines]\n",
    "\n",
    "    splitList = []\n",
    "    allToken = []\n",
    "    scoreDict = {}\n",
    "\n",
    "\n",
    "    for i in range(len(cleanedEmoLines)):\n",
    "        splitList.append(cleanedEmoLines[i].split(','))\n",
    "\n",
    "    for k in range(len(splitList)):\n",
    "        if splitList[k][2] != \"0\":\n",
    "            allToken.append(splitList[k])\n",
    "\n",
    "\n",
    "    # Score system\n",
    "    for j in allToken:\n",
    "            key = j[0]\n",
    "            if key not in scoreDict:\n",
    "                value = [j[1]]\n",
    "                scoreDict[key] = value\n",
    "            else:\n",
    "                value = j[1]\n",
    "                scoreDict[key].append(value)\n",
    "    for words in scoreDict.keys():\n",
    "        score = 0\n",
    "        for item in scoreDict[words]:\n",
    "            if (item == \"anger\" or item == \"disgust\"):\n",
    "                score -= 90\n",
    "            if (item == \"fear\" or item == \"sadness\"):\n",
    "                score -= 80\n",
    "            if (item == \"anticipation\" or item == \"surprise\"):\n",
    "                score += 25\n",
    "            if (item == \"joy\"):\n",
    "                score += 40\n",
    "            if (item == \"trust\"):\n",
    "                score += 15\n",
    "            if (item == \"negative\"):\n",
    "                score -= 100\n",
    "            if (item == \"positive\"):\n",
    "                score += 100\n",
    "            scoreDict[words] = score\n",
    "\n",
    "    return scoreDict\n",
    "            \n",
    "def baselineClassify(file, scoreDict):\n",
    "    token = cleanText(readFile(file))\n",
    "    scoreList = []\n",
    "    \n",
    "    for v in token:\n",
    "        if v in scoreDict.keys():\n",
    "            scoreList.append(scoreDict[v])  \n",
    "            \n",
    "    count = 0\n",
    "    \n",
    "    for num in scoreList:\n",
    "        if num > 0 :\n",
    "            count += 1\n",
    "            \n",
    "    if count / len(scoreList) > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Accuracy calculation\n",
    "def baselineAcc(directory):\n",
    "    acc = []\n",
    "    scoreDict = emotionDataScore()\n",
    "    \n",
    "    for file in listdir(directory):\n",
    "        path = directory + '/' + file\n",
    "        acc.append(baselineClassify(path, scoreDict))\n",
    "        \n",
    "    accuracy = round(sum(acc)/len(acc),3)\n",
    "    \n",
    "    return(accuracy)\n",
    "\n",
    "def runBaseline():\n",
    "    print(\"Negative Review Baseline Accuracy: \"+ str((baselineAcc(NEG_FILE))*100)+\" %\")\n",
    "    print(\"Positive Review Baseline Accuracy: \" + str((baselineAcc(POS_FILE)*100)) +\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression and Machine Learning Methods\n",
    "The following function uses more elaborate and complex methods to compute the sentiment. Summary results will also be printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate features\n",
    "def findFeatures(tokens):\n",
    "    words = set(tokens)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for word in words:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def get_precision(classifier):\n",
    "    classifier = SklearnClassifier(classifier)\n",
    "    classifier.train(training_set)\n",
    "    precision = (nltk.classify.accuracy(classifier, testing_set))*100\n",
    "    return precision\n",
    "\n",
    "def runTest(k, split):\n",
    "    print(\"==============================================\")\n",
    "    print(\"Running for \" +str(k) +\" most common words.\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    posCounter = Counter()\n",
    "    negCounter = Counter()\n",
    "\n",
    "    readAndClean(POS_FILE, posCounter)\n",
    "    readAndClean(NEG_FILE, negCounter)\n",
    "\n",
    "    allCounters = posCounter + negCounter\n",
    "    mostCommon = allCounters.most_common(k)\n",
    "    mostFrequentWords = []\n",
    "\n",
    "    #Drop freq\n",
    "    for i in range (0, len(mostCommon)):\n",
    "        mostFrequentWords.append(mostCommon[i][0])\n",
    "\n",
    "    positiveFiles = prepFiles(POS_FILE, 'pos')\n",
    "    negativeFiles = prepFiles(NEG_FILE, 'neg')\n",
    "    posNegFiles = positiveFiles + negativeFiles\n",
    "    \n",
    "    random.shuffle(posNegFiles)\n",
    "\n",
    "    featureSet = [(findFeatures(tokens), polarity) for (tokens, polarity) in posNegFiles]\n",
    "    trainingSet = featureSet[:split]\n",
    "    testingSet = featureSet[split:]\n",
    "    \n",
    "    #Naive Bayes\n",
    "    NaiveBayesClassifier = nltk.NaiveBayesClassifier.train(trainingSet)\n",
    "    print(\"NaiveBayes Accuracy:\", (nltk.classify.accuracy(NaiveBayesClassifier, testingSet)) * 100, '%')\n",
    "\n",
    "    #Logistic Regression\n",
    "    logisticRegressionClassifier = SklearnClassifier(LogisticRegression(solver = 'liblinear'))\n",
    "    logisticRegressionClassifier.train(trainingSet)\n",
    "    print(\"Logistic Regression(liblinear) Accuracy:\", (nltk.classify.accuracy(logisticRegressionClassifier, testingSet)) * 100, '%')\n",
    "\n",
    "    #Modifying Solver\n",
    "    logisticRegressionClassifier = SklearnClassifier(LogisticRegression(solver = 'lbfgs'))\n",
    "    logisticRegressionClassifier.train(trainingSet)\n",
    "    print(\"Logistic Regression(lbfgs) Accuracy:\", (nltk.classify.accuracy(logisticRegressionClassifier, testingSet)) * 100, '%')\n",
    "\n",
    "    #Linear SVC\n",
    "    LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "    LinearSVC_classifier.train(trainingSet)\n",
    "    print(\"Linear SVC Accuracy:\", (nltk.classify.accuracy(LinearSVC_classifier, testingSet))*100, '%')\n",
    "\n",
    "    print(\"\\nNB Classifier: \")\n",
    "    NaiveBayesClassifier.show_most_informative_features(15)\n",
    "\n",
    "def runAll():\n",
    "    print(\"==== Sentiment Analysis For Movie Review Data ====\")\n",
    "    runBaseline()\n",
    "    runTest(1000, 1600)\n",
    "    runTest(3000, 1600)\n",
    "    runTest(6000, 1600)\n",
    "    runTest(10000, 1600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - Movie Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Score Based Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Review Baseline Accuracy: 79.7 %\n",
      "Positive Review Baseline Accuracy: 92.10000000000001 %\n"
     ]
    }
   ],
   "source": [
    "runBaseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Methods with Various Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Running for 1000 most common words.\n",
      "----------------------------------------------\n",
      "NaiveBayes Accuracy: 70.5 %\n",
      "Logistic Regression(liblinear) Accuracy: 88.5 %\n",
      "Logistic Regression(lbfgs) Accuracy: 87.5 %\n",
      "Linear SVC Accuracy: 85.5 %\n",
      "\n",
      "NB Classifier: \n",
      "Most Informative Features\n",
      "               ludicrous = True              neg : pos    =     20.9 : 1.0\n",
      "                   sucks = True              neg : pos    =     14.7 : 1.0\n",
      "                   inept = True              neg : pos    =     13.8 : 1.0\n",
      "                feelgood = True              pos : neg    =     12.0 : 1.0\n",
      "               affecting = True              pos : neg    =     11.4 : 1.0\n",
      "               atrocious = True              neg : pos    =     10.6 : 1.0\n",
      "           unintentional = True              neg : pos    =      9.9 : 1.0\n",
      "                religion = True              pos : neg    =      9.9 : 1.0\n",
      "              chronicles = True              pos : neg    =      9.4 : 1.0\n",
      "                  minnie = True              pos : neg    =      9.4 : 1.0\n",
      "                segments = True              pos : neg    =      9.4 : 1.0\n",
      "             outstanding = True              pos : neg    =      9.3 : 1.0\n",
      "               stupidity = True              neg : pos    =      9.3 : 1.0\n",
      "              astounding = True              pos : neg    =      8.8 : 1.0\n",
      "                  avoids = True              pos : neg    =      8.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Top 1000 Words, 80:20 Train-Test Split\n",
    "runTest(1000, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Running for 3000 most common words.\n",
      "----------------------------------------------\n",
      "NaiveBayes Accuracy: 75.0 %\n",
      "Logistic Regression(liblinear) Accuracy: 85.25 %\n",
      "Logistic Regression(lbfgs) Accuracy: 84.25 %\n",
      "Linear SVC Accuracy: 84.0 %\n",
      "\n",
      "NB Classifier: \n",
      "Most Informative Features\n",
      "            breathtaking = True              pos : neg    =     13.5 : 1.0\n",
      "                  debate = True              pos : neg    =     12.5 : 1.0\n",
      "               maintains = True              pos : neg    =     11.8 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.6 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.3 : 1.0\n",
      "             fascination = True              pos : neg    =     11.1 : 1.0\n",
      "                captures = True              pos : neg    =     11.0 : 1.0\n",
      "                 miscast = True              neg : pos    =     10.9 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.4 : 1.0\n",
      "                  doubts = True              pos : neg    =      9.7 : 1.0\n",
      "                    gump = True              pos : neg    =      9.7 : 1.0\n",
      "                  hatred = True              pos : neg    =      9.7 : 1.0\n",
      "                palpable = True              pos : neg    =      9.7 : 1.0\n",
      "                  forgot = True              neg : pos    =      9.5 : 1.0\n",
      "                 garbage = True              neg : pos    =      9.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Top 3000 Words, 80:20 Train-Test Split\n",
    "runTest(3000, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Running for 6000 most common words.\n",
      "----------------------------------------------\n",
      "NaiveBayes Accuracy: 68.0 %\n",
      "Logistic Regression(liblinear) Accuracy: 90.25 %\n",
      "Logistic Regression(lbfgs) Accuracy: 89.75 %\n",
      "Linear SVC Accuracy: 89.0 %\n",
      "\n",
      "NB Classifier: \n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.8 : 1.0\n",
      "                chilling = True              pos : neg    =     10.7 : 1.0\n",
      "             fascination = True              pos : neg    =     10.1 : 1.0\n",
      "               performed = True              pos : neg    =     10.1 : 1.0\n",
      "                    slip = True              pos : neg    =     10.1 : 1.0\n",
      "               atrocious = True              neg : pos    =      9.9 : 1.0\n",
      "                  seagal = True              neg : pos    =      9.9 : 1.0\n",
      "              astounding = True              pos : neg    =      9.4 : 1.0\n",
      "                  avoids = True              pos : neg    =      9.4 : 1.0\n",
      "              chronicles = True              pos : neg    =      9.4 : 1.0\n",
      "                  elliot = True              pos : neg    =      9.4 : 1.0\n",
      "               fashioned = True              pos : neg    =      9.4 : 1.0\n",
      "                     dud = True              neg : pos    =      9.3 : 1.0\n",
      "              unbearable = True              neg : pos    =      9.3 : 1.0\n",
      "              unoriginal = True              neg : pos    =      9.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Top 6000 Words, 80:20 Train-Test Split\n",
    "runTest(6000, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Running for 10000 most common words.\n",
      "----------------------------------------------\n",
      "NaiveBayes Accuracy: 67.75 %\n",
      "Logistic Regression(liblinear) Accuracy: 86.75 %\n",
      "Logistic Regression(lbfgs) Accuracy: 87.25 %\n",
      "Linear SVC Accuracy: 86.5 %\n",
      "\n",
      "NB Classifier: \n",
      "Most Informative Features\n",
      "                captures = True              pos : neg    =     17.3 : 1.0\n",
      "            effortlessly = True              pos : neg    =     12.1 : 1.0\n",
      "                 offbeat = True              pos : neg    =     12.1 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.4 : 1.0\n",
      "               strongest = True              pos : neg    =     11.4 : 1.0\n",
      "                depicted = True              pos : neg    =     10.8 : 1.0\n",
      "               atrocious = True              neg : pos    =     10.6 : 1.0\n",
      "                  seagal = True              neg : pos    =     10.6 : 1.0\n",
      "             outstanding = True              pos : neg    =     10.3 : 1.0\n",
      "              astounding = True              pos : neg    =     10.1 : 1.0\n",
      "                  elliot = True              pos : neg    =     10.1 : 1.0\n",
      "               testament = True              pos : neg    =     10.1 : 1.0\n",
      "                poignant = True              pos : neg    =     10.0 : 1.0\n",
      "                   sucks = True              neg : pos    =      9.6 : 1.0\n",
      "                  hatred = True              pos : neg    =      9.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Top 10000 Words, 80:20 Train-Test Split\n",
    "runTest(10000, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78584dc55ae60dd29b7ea6026a0d9adc161362736d3288fc4461919314f638d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
